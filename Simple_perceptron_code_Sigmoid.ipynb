{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN model using backpropagation as a learning algorithm\n",
    "#### The ANN is composed of one hidden layer with one formal neuron with 2 input and 1 output variables\n",
    "#### See https://medium.com/@soudanik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Activation function : identity function\n",
    "def activ_func(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid activation function\n",
    "def activ_func_derivative(x):\n",
    "    return activ_func(x) * (1 - activ_func(x))\n",
    "\n",
    "# dataset\n",
    "X1 = np.array([1, 2, 3])\n",
    "X2 = np.array([2, 3, 7])\n",
    "Y_true = np.array([3, 5, 10])\n",
    "\n",
    "# Initialize weights and bias\n",
    "w1 = 0.5\n",
    "w2 = -0.5\n",
    "b = 0.2\n",
    "# Learning rate\n",
    "lr = 0.1 # Must be small enough to converge - Many lr values can be tested\n",
    "\n",
    "# Number of iterations for gradient descent\n",
    "iterations = 10000 # Many iterations can be tested\n",
    "\n",
    "# DataFrame to store the values\n",
    "\n",
    "df1 = pd.DataFrame(columns=['dw1', 'dw2', 'db', 'w1', 'w2', 'b'])\n",
    "\n",
    "# dw and db are the derivatives of loss function with respect to w and b respectively\n",
    "# w and b are the weights and bias respectively\n",
    "\n",
    "df2 = pd.DataFrame(columns=['loss','Z', 'Y_pred','dZ','RMSE'])\n",
    "# loss, Z, Y_pred, dZ, RMSE : are the loss, weighted sum, dZ, predicted output and root mean square error respectively\n",
    "# dZ is the derivative of loss function with respect to Z\n",
    "# Single layer model\n",
    "def single_layer_model(X1, X2, w1, w2, b):\n",
    "    Z = w1*X1 + w2*X2 + b\n",
    "    Y_pred = activ_func(Z)\n",
    "    return Y_pred, Z\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # Forward propagation\n",
    "    Y_pred, Z = single_layer_model(X1, X2, w1, w2, b)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = (1/6)*np.sum((Y_true - Y_pred)**2)\n",
    "    ## Note that loss is divided by 2 to make the derivation simpler\n",
    "    ## For RMSE, loss is multiplied by 2 and then square rooted to get the RMSE usually used in regression problems\n",
    "    RMSE = np.sqrt(loss*2)\n",
    "    \n",
    "    # Backward propagation\n",
    "    \n",
    "    dZ = (Y_pred - Y_true) * activ_func_derivative(Z)\n",
    " \n",
    "    dw1 = np.mean(dZ * X1)\n",
    "    dw2 = np.mean(dZ * X2)\n",
    "    db = np.mean(dZ)\n",
    "\n",
    "    # Update weights and bias\n",
    "    w1 -= lr * dw1\n",
    "    w2 -= lr * dw2\n",
    "    b -= lr * db\n",
    "\n",
    "    # Store the values in the DataFrame\n",
    "    df1.loc[i] = [dw1, dw2, db, w1, w2, b]\n",
    "    df2.loc[i] = [loss, Z, Y_pred, dZ, RMSE]\n",
    "\n",
    "# Plots\n",
    "## Add initialization values to the DataFrame df1 - for plotting purposes\n",
    "init_values = [0, 0, 0, 0.5, -0.5, 0.2]\n",
    "df_init = pd.DataFrame([init_values])\n",
    "df_init.columns = df1.columns\n",
    "# Concatenate df_init and df1\n",
    "df1 = pd.concat([df_init, df1]).reset_index(drop=True)\n",
    "# Plots\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))  # Create 5 subplots on one line\n",
    "\n",
    "# Plot w1\n",
    "axs[0].plot(range(iterations+1), df1['w1'])\n",
    "axs[0].set_title('W1 over iterations')\n",
    "axs[0].set_xlabel('Iterations')\n",
    "axs[0].set_ylabel('W1')\n",
    "\n",
    "# Plot w2\n",
    "axs[1].plot(range(iterations+1), df1['w2'])\n",
    "axs[1].set_title('W2 over iterations')\n",
    "axs[1].set_xlabel('Iterations')\n",
    "axs[1].set_ylabel('W2')\n",
    "\n",
    "# Plot b\n",
    "axs[2].plot(range(iterations+1), df1['b'])\n",
    "axs[2].set_title('b over iterations')\n",
    "axs[2].set_xlabel('Iterations')\n",
    "axs[2].set_ylabel('b')\n",
    "# Plot RMSE\n",
    "axs[3].plot(range(iterations), df2['RMSE'])\n",
    "axs[3].set_title('RMSE over iterations')\n",
    "axs[3].set_xlabel('Iterations')\n",
    "axs[3].set_ylabel('RMSE')\n",
    "\n",
    "# Plot Y_true vs Y_pred (get last values of Y_pred at the last iteration)\n",
    "Y_pred_final = df2['Y_pred'].tail(1).values[0]\n",
    "Y_true=np.array([3, 5, 10])\n",
    "axs[4].scatter(Y_true, Y_pred_final)\n",
    "axs[4].set_title('Predicted Y vs Observed Y')\n",
    "axs[4].set_xlabel('Observed Y')\n",
    "axs[4].set_ylabel('Predicted Y')\n",
    "\n",
    "# Calculate linear regression between predicted and observed Y\n",
    "slope, intercept = np.polyfit(Y_true, Y_pred_final, 1)\n",
    "\n",
    "# Plot regression line\n",
    "x = np.array([min(Y_true), max(Y_true)])\n",
    "y = slope * x + intercept\n",
    "axs[4].plot(x, y, color='red')\n",
    "\n",
    "# Calculate R² and RMSE\n",
    "r2 = r2_score(Y_true, Y_pred_final)\n",
    "rmse = np.sqrt(mean_squared_error(Y_true, Y_pred_final))\n",
    "\n",
    "# Show R² and RMSE on the plot\n",
    "axs[4].text(0.05, 0.95, f'R² = {r2:.2f}\\nRMSE = {rmse:.3f}', transform=axs[4].transAxes, verticalalignment='top')\n",
    "\n",
    "# Set a common title for all subplots\n",
    "fig.suptitle(\"Activation function: Sigmoid function\\nLearning rate: 0.001\\nIterations: 1000\")\n",
    "plt.tight_layout()  # Adjust the padding between and around the subplots\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
